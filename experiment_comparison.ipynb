{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®Ÿé¨“çµæœæ¯”è¼ƒãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€baseline / CLS Token / Multiscale ã®3ã¤ã®å®Ÿé¨“çµæœã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\n",
    "\n",
    "## ä½¿ã„æ–¹\n",
    "1. å„å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’ `exp_baseline`, `exp_cls`, `exp_multiscale` ã«ä¿å­˜\n",
    "2. ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š\n",
    "experiments = {\n",
    "    'baseline': '/content/exp_baseline',\n",
    "    'cls_token': '/content/exp_cls',\n",
    "    'multiscale': '/content/exp_multiscale'\n",
    "}\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ç¢ºèª\n",
    "for name, path in experiments.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ“ {name}: {path}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            level = root.replace(path, '').count(os.sep)\n",
    "            if level > 2:\n",
    "                continue\n",
    "            indent = '  ' * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = '  ' * (level + 1)\n",
    "            for f in files[:5]:\n",
    "                print(f\"{subindent}{f}\")\n",
    "            if len(files) > 5:\n",
    "                print(f\"{subindent}... (and {len(files)-5} more)\")\n",
    "    else:\n",
    "        print(f\"  âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metrics_files(directory):\n",
    "    \"\"\"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return []\n",
    "    \n",
    "    metrics_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            if f.startswith('metrics') and f.endswith('.json'):\n",
    "                metrics_files.append(os.path.join(root, f))\n",
    "    return metrics_files\n",
    "\n",
    "def load_metrics(path):\n",
    "    \"\"\"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# å„å®Ÿé¨“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿\n",
    "results = {}\n",
    "\n",
    "for name, directory in experiments.items():\n",
    "    files = find_metrics_files(directory)\n",
    "    print(f\"\\n{name}: {len(files)} å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "    \n",
    "    if files:\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "        \n",
    "        # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "        results[name] = load_metrics(files[0])\n",
    "    else:\n",
    "        print(f\"  âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãªã—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ã‚µãƒãƒªãƒ¼æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "summary_data = []\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    row = {\n",
    "        'Experiment': name,\n",
    "        'AUROC (%)': metrics.get('mean_classification_au_roc', 0) * 100,\n",
    "        'AP (%)': metrics.get('mean_classification_ap', 0) * 100,\n",
    "        'F1 (%)': metrics.get('mean_classification_f1', 0) * 100,\n",
    "    }\n",
    "    \n",
    "    # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚ã‚Œã°ï¼‰\n",
    "    if 'mean_segmentation_au_roc' in metrics:\n",
    "        row['Seg AUROC (%)'] = metrics['mean_segmentation_au_roc'] * 100\n",
    "        row['AU-PRO (%)'] = metrics.get('mean_au_pro', 0) * 100\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "if summary_data:\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    df_summary = df_summary.set_index('Experiment')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š å®Ÿé¨“ã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\"*60)\n",
    "    display(df_summary.round(2))\n",
    "    \n",
    "    # æœ€è‰¯ã®å®Ÿé¨“ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n",
    "    if len(df_summary) > 1:\n",
    "        print(\"\\nğŸ† å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§æœ€è‰¯:\")\n",
    "        for col in df_summary.columns:\n",
    "            best = df_summary[col].idxmax()\n",
    "            print(f\"  {col}: {best} ({df_summary[col].max():.2f}%)\")\n",
    "else:\n",
    "    print(\"âŒ æ¯”è¼ƒå¯èƒ½ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MVTecã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®é †åº\n",
    "mvtec_objects = [\n",
    "    \"bottle\", \"cable\", \"capsule\", \"carpet\", \"grid\", \"hazelnut\",\n",
    "    \"leather\", \"metal_nut\", \"pill\", \"screw\", \"tile\", \"toothbrush\",\n",
    "    \"transistor\", \"wood\", \"zipper\"\n",
    "]\n",
    "\n",
    "# ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥AUROCã‚’æŠ½å‡º\n",
    "object_data = []\n",
    "\n",
    "for obj in mvtec_objects:\n",
    "    row = {'Object': obj}\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        if obj in metrics and 'classification_AUROC' in metrics[obj]:\n",
    "            row[name] = metrics[obj]['classification_AUROC'] * 100\n",
    "        else:\n",
    "            row[name] = None\n",
    "    \n",
    "    object_data.append(row)\n",
    "\n",
    "if object_data and len(results) > 0:\n",
    "    df_objects = pd.DataFrame(object_data)\n",
    "    df_objects = df_objects.set_index('Object')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“‹ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ AUROC (%)\")\n",
    "    print(\"=\"*70)\n",
    "    display(df_objects.round(2))\n",
    "    \n",
    "    # å·®åˆ†åˆ†æ\n",
    "    if len(results) > 1:\n",
    "        exp_names = list(results.keys())\n",
    "        baseline = exp_names[0]\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ {baseline}ã‹ã‚‰ã®å·®åˆ†:\")\n",
    "        for exp in exp_names[1:]:\n",
    "            if exp in df_objects.columns and baseline in df_objects.columns:\n",
    "                diff = df_objects[exp] - df_objects[baseline]\n",
    "                improved = diff[diff > 0].sort_values(ascending=False)\n",
    "                degraded = diff[diff < 0].sort_values()\n",
    "                \n",
    "                print(f\"\\n  {exp}:\")\n",
    "                if len(improved) > 0:\n",
    "                    print(f\"    âœ… æ”¹å–„: {', '.join([f'{k} (+{v:.1f}%)' for k, v in improved.head(5).items()])}\")\n",
    "                if len(degraded) > 0:\n",
    "                    print(f\"    âš ï¸ ä½ä¸‹: {', '.join([f'{k} ({v:.1f}%)' for k, v in degraded.head(5).items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if len(results) > 0 and len(object_data) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # ã‚°ãƒ©ãƒ•1: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥AUROCæ£’ã‚°ãƒ©ãƒ•\n",
    "    ax1 = axes[0]\n",
    "    x = np.arange(len(mvtec_objects))\n",
    "    width = 0.25\n",
    "    \n",
    "    exp_names = list(results.keys())\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    \n",
    "    for i, (name, color) in enumerate(zip(exp_names, colors)):\n",
    "        if name in df_objects.columns:\n",
    "            values = df_objects[name].fillna(0).values\n",
    "            ax1.bar(x + i * width, values, width, label=name, color=color, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Object')\n",
    "    ax1.set_ylabel('AUROC (%)')\n",
    "    ax1.set_title('Object-wise AUROC Comparison')\n",
    "    ax1.set_xticks(x + width)\n",
    "    ax1.set_xticklabels(mvtec_objects, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim([50, 102])\n",
    "    ax1.axhline(y=90, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ã‚°ãƒ©ãƒ•2: å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ\n",
    "    ax2 = axes[1]\n",
    "    metrics_names = ['AUROC', 'AP', 'F1']\n",
    "    x2 = np.arange(len(metrics_names))\n",
    "    \n",
    "    for i, (name, color) in enumerate(zip(exp_names, colors)):\n",
    "        if name in results:\n",
    "            values = [\n",
    "                results[name].get('mean_classification_au_roc', 0) * 100,\n",
    "                results[name].get('mean_classification_ap', 0) * 100,\n",
    "                results[name].get('mean_classification_f1', 0) * 100,\n",
    "            ]\n",
    "            ax2.bar(x2 + i * width, values, width, label=name, color=color, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Metric')\n",
    "    ax2.set_ylabel('Score (%)')\n",
    "    ax2.set_title('Mean Classification Metrics')\n",
    "    ax2.set_xticks(x2 + width)\n",
    "    ax2.set_xticklabels(metrics_names)\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim([80, 100])\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¾ ã‚°ãƒ©ãƒ•ã‚’ experiment_comparison.png ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: å•é¡Œã‚«ãƒ†ã‚´ãƒªã®è©³ç´°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•é¡ŒãŒã‚ã‚‹ã‚«ãƒ†ã‚´ãƒªï¼ˆAUROC < 90%ï¼‰ã‚’ç‰¹å®š\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ” å•é¡Œã‚«ãƒ†ã‚´ãƒªåˆ†æ (AUROC < 90%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\nã€{name}ã€‘\")\n",
    "    problem_objects = []\n",
    "    \n",
    "    for obj in mvtec_objects:\n",
    "        if obj in metrics and 'classification_AUROC' in metrics[obj]:\n",
    "            auroc = metrics[obj]['classification_AUROC'] * 100\n",
    "            if auroc < 90:\n",
    "                problem_objects.append((obj, auroc))\n",
    "    \n",
    "    if problem_objects:\n",
    "        for obj, auroc in sorted(problem_objects, key=lambda x: x[1]):\n",
    "            status = \"âš ï¸\" if auroc >= 70 else \"âŒ\"\n",
    "            print(f\"  {status} {obj}: {auroc:.1f}%\")\n",
    "    else:\n",
    "        print(\"  âœ… å…¨ã‚«ãƒ†ã‚´ãƒª 90%ä»¥ä¸Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: çµè«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ çµè«–\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(results) > 1:\n",
    "    exp_names = list(results.keys())\n",
    "    baseline = exp_names[0]\n",
    "    baseline_auroc = results[baseline].get('mean_classification_au_roc', 0) * 100\n",
    "    \n",
    "    print(f\"\\nãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ ({baseline}): AUROC {baseline_auroc:.2f}%\")\n",
    "    print(\"\\nå„æ‰‹æ³•ã®åŠ¹æœ:\")\n",
    "    \n",
    "    for exp in exp_names[1:]:\n",
    "        if exp in results:\n",
    "            exp_auroc = results[exp].get('mean_classification_au_roc', 0) * 100\n",
    "            diff = exp_auroc - baseline_auroc\n",
    "            \n",
    "            if diff > 0.1:\n",
    "                print(f\"  âœ… {exp}: +{diff:.2f}% (AUROC: {exp_auroc:.2f}%)\")\n",
    "            elif diff < -0.1:\n",
    "                print(f\"  âš ï¸ {exp}: {diff:.2f}% (AUROC: {exp_auroc:.2f}%)\")\n",
    "            else:\n",
    "                print(f\"  â¡ï¸ {exp}: Â±{abs(diff):.2f}% (AUROC: {exp_auroc:.2f}%) - å¤‰åŒ–ãªã—\")\n",
    "    \n",
    "    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
    "    best_exp = max(results.items(), key=lambda x: x[1].get('mean_classification_au_roc', 0))\n",
    "    print(f\"\\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_exp[0]} (AUROC: {best_exp[1].get('mean_classification_au_roc', 0)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\næ¯”è¼ƒå¯¾è±¡ãŒ1ã¤ã®ã¿ã§ã™ã€‚è¤‡æ•°ã®å®Ÿé¨“çµæœã‚’ç”¨æ„ã—ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
