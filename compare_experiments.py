#!/usr/bin/env python3
"""
å®Ÿé¨“çµæœæ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦æ¯”è¼ƒ
    python compare_experiments.py --dirs exp_baseline exp_cls exp_multiscale

    # Colabä¸Šã§ä½¿ç”¨ã™ã‚‹å ´åˆ
    from compare_experiments import compare_experiments
    compare_experiments({
        'baseline': '/content/exp_baseline',
        'cls_token': '/content/exp_cls',
        'multiscale': '/content/exp_multiscale'
    })
"""

import json
import os
import argparse
from pathlib import Path
from collections import defaultdict


def find_metrics_file(directory):
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ:
    - metrics_seed=0.json (ç›´æ¥)
    - metrics_seed=*.json (ä»»æ„ã®ã‚·ãƒ¼ãƒ‰)
    - ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…
    """
    directory = Path(directory)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç›´æ¥é…ä¸‹
    for pattern in ['metrics_seed=*.json', 'metrics.json']:
        files = list(directory.glob(pattern))
        if files:
            return files

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: 1éšå±¤ä¸‹
    for pattern in ['*/metrics_seed=*.json', '*/metrics.json']:
        files = list(directory.glob(pattern))
        if files:
            return files

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: 2éšå±¤ä¸‹ (results_MVTec/model/shot-preprocess/metrics.json)
    for pattern in ['**/metrics_seed=*.json', '**/metrics.json']:
        files = list(directory.glob(pattern))
        if files:
            return files[:5]  # å¤šã™ãã‚‹å ´åˆã¯æœ€åˆã®5ã¤

    return []


def load_metrics(metrics_path):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(metrics_path, 'r') as f:
        return json.load(f)


def compare_experiments(experiment_dirs, show_per_object=False, show_diff=True):
    """
    è¤‡æ•°ã®å®Ÿé¨“çµæœã‚’æ¯”è¼ƒ

    Args:
        experiment_dirs: dict {å®Ÿé¨“å: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹}
        show_per_object: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ã®è©³ç´°ã‚’è¡¨ç¤º
        show_diff: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®å·®åˆ†ã‚’è¡¨ç¤º
    """
    results = {}
    all_objects = set()

    print("=" * 70)
    print("å®Ÿé¨“çµæœã®æ¯”è¼ƒ")
    print("=" * 70)

    # å„å®Ÿé¨“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
    for name, directory in experiment_dirs.items():
        metrics_files = find_metrics_file(directory)

        if not metrics_files:
            print(f"âš ï¸  {name}: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ ({directory})")
            print(f"   ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹: {list(Path(directory).glob('*'))[:10]}")
            continue

        # è¤‡æ•°ã‚·ãƒ¼ãƒ‰ã®å ´åˆã¯å¹³å‡ã‚’å–ã‚‹
        all_metrics = []
        for mf in metrics_files:
            try:
                metrics = load_metrics(mf)
                all_metrics.append(metrics)
                print(f"âœ“ {name}: {mf}")
            except Exception as e:
                print(f"âš ï¸  {name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({mf}): {e}")

        if not all_metrics:
            continue

        # å¹³å‡ã‚’è¨ˆç®—
        averaged = {}
        keys = all_metrics[0].keys()

        for key in keys:
            if key.startswith('mean_'):
                # æ•°å€¤ã®å¹³å‡
                values = [m[key] for m in all_metrics if key in m]
                averaged[key] = sum(values) / len(values) if values else 0
            elif isinstance(all_metrics[0].get(key), dict):
                # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                all_objects.add(key)
                averaged[key] = {}
                for metric_name in all_metrics[0][key].keys():
                    values = [m[key][metric_name] for m in all_metrics if key in m]
                    averaged[key][metric_name] = sum(values) / len(values) if values else 0

        results[name] = averaged

    if not results:
        print("\nâŒ æ¯”è¼ƒå¯èƒ½ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return None

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "=" * 70)
    print("ğŸ“Š ã‚µãƒãƒªãƒ¼ (å¹³å‡å€¤)")
    print("=" * 70)

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    exp_names = list(results.keys())
    header = f"{'Metric':<25}"
    for name in exp_names:
        header += f" | {name:>12}"
    if show_diff and len(exp_names) > 1:
        header += f" | {'Diff':>10}"
    print(header)
    print("-" * len(header))

    # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    metrics_to_show = [
        ('mean_classification_au_roc', 'AUROC (åˆ†é¡)'),
        ('mean_classification_ap', 'AP (åˆ†é¡)'),
        ('mean_classification_f1', 'F1 (åˆ†é¡)'),
        ('mean_segmentation_au_roc', 'AUROC (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ)'),
        ('mean_segmentation_f1', 'F1 (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ)'),
        ('mean_au_pro', 'AU-PRO'),
    ]

    baseline_name = exp_names[0] if exp_names else None

    for metric_key, metric_label in metrics_to_show:
        if not any(metric_key in results[name] for name in exp_names):
            continue

        row = f"{metric_label:<25}"
        baseline_val = None

        for name in exp_names:
            val = results[name].get(metric_key, None)
            if val is not None:
                if name == baseline_name:
                    baseline_val = val
                row += f" | {val*100:>11.2f}%"
            else:
                row += f" | {'N/A':>12}"

        # å·®åˆ†è¡¨ç¤º
        if show_diff and len(exp_names) > 1 and baseline_val is not None:
            last_val = results[exp_names[-1]].get(metric_key, None)
            if last_val is not None:
                diff = (last_val - baseline_val) * 100
                diff_str = f"+{diff:.2f}%" if diff >= 0 else f"{diff:.2f}%"
                row += f" | {diff_str:>10}"

        print(row)

    # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è©³ç´°
    if show_per_object and all_objects:
        print("\n" + "=" * 70)
        print("ğŸ“‹ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ AUROC")
        print("=" * 70)

        # ã‚½ãƒ¼ãƒˆ (MVTecé †)
        mvtec_order = ["bottle", "cable", "capsule", "carpet", "grid", "hazelnut",
                       "leather", "metal_nut", "pill", "screw", "tile", "toothbrush",
                       "transistor", "wood", "zipper"]
        sorted_objects = sorted(all_objects,
                               key=lambda x: mvtec_order.index(x) if x in mvtec_order else 100)

        header = f"{'Object':<15}"
        for name in exp_names:
            header += f" | {name:>12}"
        if show_diff and len(exp_names) > 1:
            header += f" | {'Best':>10}"
        print(header)
        print("-" * len(header))

        for obj in sorted_objects:
            if obj.startswith('mean_'):
                continue

            row = f"{obj:<15}"
            values = []

            for name in exp_names:
                if obj in results[name]:
                    val = results[name][obj].get('classification_AUROC', None)
                    if val is not None:
                        values.append((name, val))
                        row += f" | {val*100:>11.2f}%"
                    else:
                        row += f" | {'N/A':>12}"
                else:
                    row += f" | {'N/A':>12}"

            # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¯
            if show_diff and len(values) > 1:
                best_name = max(values, key=lambda x: x[1])[0]
                row += f" | {best_name[:10]:>10}"

            print(row)

    return results


def debug_directory(directory):
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º"""
    print(f"\nğŸ” ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªèª¿æŸ»: {directory}")
    print("-" * 50)

    if not os.path.exists(directory):
        print(f"  âŒ å­˜åœ¨ã—ã¾ã›ã‚“")
        return

    for root, dirs, files in os.walk(directory):
        level = root.replace(directory, '').count(os.sep)
        indent = '  ' * level
        print(f"{indent}{os.path.basename(root)}/")

        # æœ€å¤§2éšå±¤ã¾ã§
        if level >= 2:
            continue

        subindent = '  ' * (level + 1)
        for file in files[:10]:  # æœ€å¤§10ãƒ•ã‚¡ã‚¤ãƒ«
            print(f"{subindent}{file}")
        if len(files) > 10:
            print(f"{subindent}... (and {len(files) - 10} more files)")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å®Ÿé¨“çµæœã®æ¯”è¼ƒ')
    parser.add_argument('--dirs', nargs='+', required=True,
                        help='æ¯”è¼ƒã™ã‚‹å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ä¾‹: exp_baseline exp_cls)')
    parser.add_argument('--names', nargs='+', default=None,
                        help='å®Ÿé¨“ã®åå‰ (æŒ‡å®šã—ãªã„å ´åˆã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½¿ç”¨)')
    parser.add_argument('--per-object', action='store_true',
                        help='ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ã®è©³ç´°ã‚’è¡¨ç¤º')
    parser.add_argument('--debug', action='store_true',
                        help='ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º')

    args = parser.parse_args()

    if args.debug:
        for d in args.dirs:
            debug_directory(d)

    names = args.names if args.names else [os.path.basename(d) for d in args.dirs]
    experiment_dirs = dict(zip(names, args.dirs))

    compare_experiments(experiment_dirs, show_per_object=args.per_object)
